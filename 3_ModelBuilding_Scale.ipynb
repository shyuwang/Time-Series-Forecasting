{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1FW4o1utqeNHyJ1f4CJ5K_TkNLhGpzSvE",
      "authorship_tag": "ABX9TyPrgtlb+Wx62F0j/sgtRRFE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shyuwang/Time-Series-Forecasting/blob/main/3_ModelBuilding_Scale.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "olo4N8_iOEnx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bbec5c06-d9a8-4909-def2-bdcae47872c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyspark\n",
            "  Downloading pyspark-3.3.2.tar.gz (281.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m281.4/281.4 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting py4j==0.10.9.5\n",
            "  Downloading py4j-0.10.9.5-py2.py3-none-any.whl (199 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.7/199.7 KB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.3.2-py2.py3-none-any.whl size=281824025 sha256=88673f30d0a5bbb37558dc2849aefb98ab8f2e64c832d6f4b4d789751a7ea421\n",
            "  Stored in directory: /root/.cache/pip/wheels/b1/59/a0/a1a0624b5e865fd389919c1a10f53aec9b12195d6747710baf\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9.5 pyspark-3.3.2\n"
          ]
        }
      ],
      "source": [
        "# Install pyspark\n",
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import SparkSession\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Create a Spark Session\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "\n",
        "# Check Spark Session Information\n",
        "# spark"
      ],
      "metadata": {
        "id": "NYJN_QJBYShW"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import *\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount google drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Path of training set on Google drive\n",
        "train_path = '/content/drive/MyDrive/demand-forecasting-at-scale-data/train.csv'\n",
        " \n",
        "# Structure of the training data set\n",
        "train_schema = StructType([\n",
        "  StructField('Date', DateType()),\n",
        "  StructField('Store', IntegerType()),\n",
        "  StructField('Item', IntegerType()),\n",
        "  StructField('Sales', IntegerType())\n",
        "  ])\n",
        " \n",
        "# Read the training file into a dataframe\n",
        "train = spark.read.csv(\n",
        "  train_path, \n",
        "  header=True, \n",
        "  schema=train_schema\n",
        "  )\n",
        "\n",
        "train.createOrReplaceTempView('train')"
      ],
      "metadata": {
        "id": "Rai_-4BzOWae",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27a42232-f636-4902-a23a-e8aa2aea771c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1:\n",
        "Retrieve data for all available store-item combinations at day level."
      ],
      "metadata": {
        "id": "TfTXck23aByA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"\"\"\n",
        "select\n",
        "  Store as store\n",
        "  ,Item as item\n",
        "  ,cast(Date as date) as ds\n",
        "  ,Sales as y\n",
        "from train\n",
        "order by 1,2,3\n",
        "\"\"\"\n",
        "\n",
        "store_item_history = spark.sql(query)"
      ],
      "metadata": {
        "id": "ZiJGYExfaOd0"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "store_item_history.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N9h3tpwoiyW-",
        "outputId": "c840993b-fab0-4373-bd1d-0c091526f391"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+----+----------+---+\n",
            "|store|item|        ds|  y|\n",
            "+-----+----+----------+---+\n",
            "|    1|   1|2013-01-01| 13|\n",
            "|    1|   1|2013-01-02| 11|\n",
            "|    1|   1|2013-01-03| 14|\n",
            "|    1|   1|2013-01-04| 13|\n",
            "|    1|   1|2013-01-05| 10|\n",
            "+-----+----+----------+---+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2:\n",
        "Define schema for forecast output.\n",
        "\n",
        "The goal is to build a model for each store-item combination and get the forecast back for this store-item subset. with that being said, we expect the returned forecast has the following fields:\n",
        "- store\n",
        "- item\n",
        "- ds\n",
        "- y (for historical data only)\n",
        "- yhat, yhat_lower, yhat_upper"
      ],
      "metadata": {
        "id": "s-_3LhN2araa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import *\n",
        "\n",
        "result_schema = StructType([\n",
        "    StructField('ds', DateType()),\n",
        "    StructField('store',IntegerType()),\n",
        "    StructField('item', IntegerType()),\n",
        "    StructField('y', FloatType()),\n",
        "    StructField('yhat',FloatType()),\n",
        "    StructField('yhat_upper',FloatType()),\n",
        "    StructField('yhat_lower',FloatType())\n",
        "])"
      ],
      "metadata": {
        "id": "f4CrLJT_alyo"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3:\n",
        "Define function to train data and forecast.\n",
        "\n",
        "The function serves to receive a single store-item combination, and return a forecast in the previously defined result schema."
      ],
      "metadata": {
        "id": "Aec7YTgJdFl4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from prophet import Prophet"
      ],
      "metadata": {
        "id": "GC9Hoa2zf8t7"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def forecast_store_item(history_pd):\n",
        "  \"\"\"\n",
        "  Receive a single store-item combination, \n",
        "  and return a forecast in the previously defined result schema.\n",
        "  \"\"\"\n",
        "  # TRAIN MODEL\n",
        "  # --------------------------------------\n",
        "  # Configure\n",
        "  model = Prophet(\n",
        "      interval_width=0.95,\n",
        "      growth='linear',\n",
        "      daily_seasonality=False,\n",
        "      weekly_seasonality=True,\n",
        "      yearly_seasonality=True,\n",
        "      seasonality_mode='multiplicative'\n",
        "  )\n",
        "\n",
        "  # Train\n",
        "  model.fit(history_pd)\n",
        "\n",
        "  # BUILD FORECAST\n",
        "  # -----------------------------------\n",
        "  future_pd = model.make_future_dataframe(\n",
        "      periods=90,\n",
        "      freq='d',\n",
        "      include_history=True\n",
        "  )\n",
        "  forecast_pd = model.predict(future_pd)\n",
        "\n",
        "  # ASSEMBLE RESULTS\n",
        "  #-------------------------------------\n",
        "  # Get relevant fields from forecast\n",
        "  f_pd = forecast_pd[['ds','yhat','yhat_upper','yhat_lower']].set_index('ds')\n",
        "  # Get relevant fields from history\n",
        "  h_pd = history_pd[['ds','store','item','y']].set_index('ds')\n",
        "\n",
        "  result_pd = f_pd.join(h_pd, how='left')\n",
        "  result_pd.reset_index(level=0, inplace=True)\n",
        "\n",
        "  # Get store and item for this subset\n",
        "  result_pd['store'] = history_pd['store'].iloc[0]\n",
        "  result_pd['item'] = history_pd['item'].iloc[0]\n",
        "\n",
        "  # Adjust the sequence and return\n",
        "  return result_pd[['ds','store','item','y','yhat','yhat_upper','yhat_lower']]"
      ],
      "metadata": {
        "id": "F6beO4iCdDSW"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4:\n",
        "Apply the pandas function to each store-item combination.\n",
        "\n",
        "1. Group the historical dataset by store, item\n",
        "2. Apply the function to each group"
      ],
      "metadata": {
        "id": "hpIqjwuThq9Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import current_date"
      ],
      "metadata": {
        "id": "3cyYZV89h-Pf"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = (\n",
        "    store_item_history\n",
        "    .groupBy('store','item')\n",
        "    .applyInPandas(forecast_store_item, schema=result_schema)\n",
        "    # add training date for data management\n",
        "    .withColumn('training_date', current_date())\n",
        ")\n",
        "\n",
        "results.createOrReplaceTempView('new_forecasts')"
      ],
      "metadata": {
        "id": "yNzR_FAJiBVj"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Glimpse of the forecast\n",
        "results.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7_oCI7cjiXh6",
        "outputId": "efd40a31-a170-4acb-e03f-1d0fd2f5b6bf"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----+----+----+---------+----------+----------+-------------+\n",
            "|        ds|store|item|   y|     yhat|yhat_upper|yhat_lower|training_date|\n",
            "+----------+-----+----+----+---------+----------+----------+-------------+\n",
            "|2013-01-01|    1|   1|13.0|10.048304| 18.312273| 1.1078302|   2023-02-17|\n",
            "|2013-01-02|    1|   1|11.0|10.525652| 18.830145| 2.6071098|   2023-02-17|\n",
            "|2013-01-03|    1|   1|14.0|11.050407|  19.78737| 2.5539734|   2023-02-17|\n",
            "|2013-01-04|    1|   1|13.0|12.241633| 20.948008|   4.42211|   2023-02-17|\n",
            "|2013-01-05|    1|   1|10.0|13.777667| 23.248283|  5.464106|   2023-02-17|\n",
            "+----------+-----+----+----+---------+----------+----------+-------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Persist forecast output to delta table if in Databricks environment."
      ],
      "metadata": {
        "id": "NyzNShXemrAP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 5:\n",
        "Evaluate model for each store-item combination."
      ],
      "metadata": {
        "id": "Y1w4OY_Omh_j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "eQv3DLsWn7fc"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation metrics schema\n",
        "eval_schema = StructType([\n",
        "    StructField('training_date', DateType()),\n",
        "    StructField('store', IntegerType()),\n",
        "    StructField('item', IntegerType()),\n",
        "    StructField('mae', FloatType()),\n",
        "    StructField('mse', FloatType()),\n",
        "    StructField('rmse', FloatType())\n",
        "])"
      ],
      "metadata": {
        "id": "vUKmgsRQm6fb"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pandas.core.aggregation import maybe_mangle_lambdas\n",
        "def evaluate_forecast(evaluation_pd):\n",
        "  \"\"\"\n",
        "  \"\"\"\n",
        "  # Retrieve information\n",
        "  training_date = evaluation_pd['training_date'].iloc[0]\n",
        "  store = evaluation_pd['store'].iloc[0]\n",
        "  item = evaluation_pd['item'].iloc[0]\n",
        "\n",
        "  # Calculate metrics\n",
        "  mae = mean_absolute_error(evaluation_pd['y'], evaluation_pd['yhat'])\n",
        "  mse = mean_squared_error(evaluation_pd['y'], evaluation_pd['yhat'])\n",
        "  rmse = mse**0.5\n",
        "\n",
        "  # Assemble results\n",
        "  result = pd.DataFrame({\"training_date\":[training_date],\"store\":[store],\"item\":[item],\"mae\":[mae],\"mse\":[mse],\"rmse\":[rmse]})\n",
        "  return result"
      ],
      "metadata": {
        "id": "XEV2RLV_naop"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = (\n",
        "    spark\n",
        "    .table('new_forecasts')\n",
        "    # limit to periods where historical data is available\n",
        "    .filter(\"ds<'2018-01-01'\")\n",
        "    .select('training_date','store','item','y','yhat')\n",
        "    .groupBy('training_date','store','item')\n",
        "    .applyInPandas(evaluate_forecast, schema=eval_schema)\n",
        ")\n",
        "\n",
        "results.createOrReplaceTempView('new_forecast_evals')\n",
        "results.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zdbAmxPFoXfi",
        "outputId": "e4dc1a83-a5f0-446e-a4d7-16c83c5a9913"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+-----+----+---------+---------+---------+\n",
            "|training_date|store|item|      mae|      mse|     rmse|\n",
            "+-------------+-----+----+---------+---------+---------+\n",
            "|   2023-02-17|    1|   1|3.4865103|19.388674|4.4032574|\n",
            "|   2023-02-17|    1|   2|6.0576615| 58.63383|7.6572733|\n",
            "|   2023-02-17|    1|   5|3.1745038|16.208797|4.0260153|\n",
            "|   2023-02-17|    1|  10|6.9078736|74.809814| 8.649266|\n",
            "|   2023-02-17|    1|  11|6.5393095| 67.67937|8.2267475|\n",
            "|   2023-02-17|    1|  13|7.2633123|82.972534| 9.108926|\n",
            "|   2023-02-17|    1|  14|  6.04398| 59.03582| 7.683477|\n",
            "|   2023-02-17|    1|  15|7.6815734|93.604546| 9.674944|\n",
            "|   2023-02-17|    1|  18|7.4220057| 86.89475| 9.321735|\n",
            "|   2023-02-17|    1|  20|5.2182527|43.944183|6.6290407|\n",
            "|   2023-02-17|    1|  26|5.3457084| 45.03836|6.7110624|\n",
            "|   2023-02-17|    1|  28| 7.646754|93.070175| 9.647288|\n",
            "|   2023-02-17|    1|  29|6.4875183| 65.63709| 8.101672|\n",
            "|   2023-02-17|    1|  30|4.8911767|  37.9981|  6.16426|\n",
            "|   2023-02-17|    1|  31| 5.894637|55.946762| 7.479757|\n",
            "|   2023-02-17|    1|  34| 3.796969|23.262266| 4.823097|\n",
            "|   2023-02-17|    1|  35| 6.384053| 65.50114| 8.093277|\n",
            "|   2023-02-17|    1|  36|6.8342223| 74.77299| 8.647138|\n",
            "|   2023-02-17|    1|  38|7.2556796| 85.36877| 9.239522|\n",
            "|   2023-02-17|    1|  39|5.1809688|42.916096| 6.551038|\n",
            "+-------------+-----+----+---------+---------+---------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Persist forecast evaluation to delta table if in Databricks environment."
      ],
      "metadata": {
        "id": "g1PF52NXpytZ"
      }
    }
  ]
}